{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m65 packages\u001b[0m \u001b[2min 25ms\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mIgnoring dangling temporary directory: `\u001b[36m/mnt/d/SFSU/CSC871/csc865-anti-money-laundering-ibm/.venv/lib/python3.12/site-packages/~ympy-1.14.0.dist-info\u001b[39m`\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mIgnoring dangling temporary directory: `\u001b[36m/mnt/d/SFSU/CSC871/csc865-anti-money-laundering-ibm/.venv/lib/python3.12/site-packages/~ympy-1.14.0.dist-info\u001b[39m`\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m30 packages\u001b[0m \u001b[2min 23.03s\u001b[0m\u001b[0m\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/2] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[2mUninstalled \u001b[1m30 packages\u001b[0m \u001b[2min 23.03s\u001b[0m\u001b[0m\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/2] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 6.98s\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1masttokens\u001b[0m\u001b[2m==3.0.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mcomm\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mdebugpy\u001b[0m\u001b[2m==1.8.17\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mdecorator\u001b[0m\u001b[2m==5.2.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mexecuting\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mipykernel\u001b[0m\u001b[2m==7.1.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mipython\u001b[0m\u001b[2m==9.7.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mipython-pygments-lexers\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mjedi\u001b[0m\u001b[2m==0.19.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==8.6.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mjupyter-core\u001b[0m\u001b[2m==5.9.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mmatplotlib\u001b[0m\u001b[2m==3.8.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmatplotlib\u001b[0m\u001b[2m==3.10.7\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mmatplotlib-inline\u001b[0m\u001b[2m==0.2.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnest-asyncio\u001b[0m\u001b[2m==1.6.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.3.5\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mparso\u001b[0m\u001b[2m==0.8.5\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpexpect\u001b[0m\u001b[2m==4.9.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpip\u001b[0m\u001b[2m==25.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mplatformdirs\u001b[0m\u001b[2m==4.5.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mprompt-toolkit\u001b[0m\u001b[2m==3.0.52\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mptyprocess\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpure-eval\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpygments\u001b[0m\u001b[2m==2.19.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==27.1.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mseaborn\u001b[0m\u001b[2m==0.13.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mstack-data\u001b[0m\u001b[2m==0.6.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtornado\u001b[0m\u001b[2m==6.5.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtraitlets\u001b[0m\u001b[2m==5.14.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mwcwidth\u001b[0m\u001b[2m==0.2.14\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mwheel\u001b[0m\u001b[2m==0.45.1\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 6.98s\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1masttokens\u001b[0m\u001b[2m==3.0.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mcomm\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mdebugpy\u001b[0m\u001b[2m==1.8.17\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mdecorator\u001b[0m\u001b[2m==5.2.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mexecuting\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mipykernel\u001b[0m\u001b[2m==7.1.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mipython\u001b[0m\u001b[2m==9.7.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mipython-pygments-lexers\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mjedi\u001b[0m\u001b[2m==0.19.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==8.6.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mjupyter-core\u001b[0m\u001b[2m==5.9.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mmatplotlib\u001b[0m\u001b[2m==3.8.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmatplotlib\u001b[0m\u001b[2m==3.10.7\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mmatplotlib-inline\u001b[0m\u001b[2m==0.2.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnest-asyncio\u001b[0m\u001b[2m==1.6.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.3.5\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mparso\u001b[0m\u001b[2m==0.8.5\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpexpect\u001b[0m\u001b[2m==4.9.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpip\u001b[0m\u001b[2m==25.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mplatformdirs\u001b[0m\u001b[2m==4.5.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mprompt-toolkit\u001b[0m\u001b[2m==3.0.52\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mptyprocess\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpure-eval\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpygments\u001b[0m\u001b[2m==2.19.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==27.1.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mseaborn\u001b[0m\u001b[2m==0.13.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mstack-data\u001b[0m\u001b[2m==0.6.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtornado\u001b[0m\u001b[2m==6.5.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtraitlets\u001b[0m\u001b[2m==5.14.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mwcwidth\u001b[0m\u001b[2m==0.2.14\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mwheel\u001b[0m\u001b[2m==0.45.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environment ===\n",
      "Platform: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.31\n",
      "Python: 3.12.11\n",
      "\n",
      "=== PyTorch / CUDA Info ===\n",
      "torch.__version__: 2.9.1+cu128\n",
      "torch.version.cuda: 12.8\n",
      "torch.cuda.is_available(): True\n",
      "torch.cuda.device_count(): 1\n",
      "  device 0: NVIDIA GeForce RTX 5080\n",
      "\n",
      "=== PyTorch / CUDA Info ===\n",
      "torch.__version__: 2.9.1+cu128\n",
      "torch.version.cuda: 12.8\n",
      "torch.cuda.is_available(): True\n",
      "torch.cuda.device_count(): 1\n",
      "  device 0: NVIDIA GeForce RTX 5080\n",
      "\n",
      "Successfully ran a matrix multiply on CUDA.\n",
      "z.device: cuda:0\n",
      "\n",
      "=== Test Complete ===\n",
      "\n",
      "Successfully ran a matrix multiply on CUDA.\n",
      "z.device: cuda:0\n",
      "\n",
      "=== Test Complete ===\n"
     ]
    }
   ],
   "source": [
    "# save as test_cuda.py and run: python3 test_cuda.py\n",
    "\n",
    "import platform\n",
    "\n",
    "print(\"=== Environment ===\")\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"Python:\", platform.python_version())\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except ImportError as e:\n",
    "    print(\"\\nPyTorch is not installed or not in this Python environment.\")\n",
    "    raise SystemExit(e)\n",
    "\n",
    "print(\"\\n=== PyTorch / CUDA Info ===\")\n",
    "print(\"torch.__version__:\", torch.__version__)\n",
    "print(\"torch.version.cuda:\", torch.version.cuda)\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"torch.cuda.is_available():\", cuda_available)\n",
    "\n",
    "if not cuda_available:\n",
    "    print(\"\\nCUDA is NOT available to PyTorch in this environment.\")\n",
    "else:\n",
    "    # Number of devices\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(\"torch.cuda.device_count():\", device_count)\n",
    "\n",
    "    for i in range(device_count):\n",
    "        print(f\"  device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "    # Simple tensor test on GPU\n",
    "    try:\n",
    "        x = torch.rand(3, 3, device=\"cuda\")\n",
    "        y = torch.rand(3, 3, device=\"cuda\")\n",
    "        z = x @ y\n",
    "        print(\"\\nSuccessfully ran a matrix multiply on CUDA.\")\n",
    "        print(\"z.device:\", z.device)\n",
    "    except Exception as e:\n",
    "        print(\"\\nERROR: Allocation or compute on CUDA failed:\")\n",
    "        print(e)\n",
    "\n",
    "print(\"\\n=== Test Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cu128\n",
      "12.8\n",
      "91002\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5078345 rows; columns: ['Timestamp', 'From Bank', 'Account', 'To Bank', 'Account.1', 'Amount Received', 'Receiving Currency', 'Amount Paid', 'Payment Currency', 'Payment Format', 'Is Laundering']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>From Bank</th>\n",
       "      <th>Account</th>\n",
       "      <th>To Bank</th>\n",
       "      <th>Account.1</th>\n",
       "      <th>Amount Received</th>\n",
       "      <th>Receiving Currency</th>\n",
       "      <th>Amount Paid</th>\n",
       "      <th>Payment Currency</th>\n",
       "      <th>Payment Format</th>\n",
       "      <th>Is Laundering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022/09/01 00:20</td>\n",
       "      <td>10</td>\n",
       "      <td>8000EBD30</td>\n",
       "      <td>10</td>\n",
       "      <td>8000EBD30</td>\n",
       "      <td>3697.34</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>3697.34</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022/09/01 00:20</td>\n",
       "      <td>3208</td>\n",
       "      <td>8000F4580</td>\n",
       "      <td>1</td>\n",
       "      <td>8000F5340</td>\n",
       "      <td>0.01</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>0.01</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Cheque</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022/09/01 00:00</td>\n",
       "      <td>3209</td>\n",
       "      <td>8000F4670</td>\n",
       "      <td>3209</td>\n",
       "      <td>8000F4670</td>\n",
       "      <td>14675.57</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>14675.57</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022/09/01 00:02</td>\n",
       "      <td>12</td>\n",
       "      <td>8000F5030</td>\n",
       "      <td>12</td>\n",
       "      <td>8000F5030</td>\n",
       "      <td>2806.97</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>2806.97</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022/09/01 00:06</td>\n",
       "      <td>10</td>\n",
       "      <td>8000F5200</td>\n",
       "      <td>10</td>\n",
       "      <td>8000F5200</td>\n",
       "      <td>36682.97</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>36682.97</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Timestamp  From Bank    Account  To Bank  Account.1  \\\n",
       "0  2022/09/01 00:20         10  8000EBD30       10  8000EBD30   \n",
       "1  2022/09/01 00:20       3208  8000F4580        1  8000F5340   \n",
       "2  2022/09/01 00:00       3209  8000F4670     3209  8000F4670   \n",
       "3  2022/09/01 00:02         12  8000F5030       12  8000F5030   \n",
       "4  2022/09/01 00:06         10  8000F5200       10  8000F5200   \n",
       "\n",
       "   Amount Received Receiving Currency  Amount Paid Payment Currency  \\\n",
       "0          3697.34          US Dollar      3697.34        US Dollar   \n",
       "1             0.01          US Dollar         0.01        US Dollar   \n",
       "2         14675.57          US Dollar     14675.57        US Dollar   \n",
       "3          2806.97          US Dollar      2806.97        US Dollar   \n",
       "4         36682.97          US Dollar     36682.97        US Dollar   \n",
       "\n",
       "  Payment Format  Is Laundering  \n",
       "0   Reinvestment              0  \n",
       "1         Cheque              0  \n",
       "2   Reinvestment              0  \n",
       "3   Reinvestment              0  \n",
       "4   Reinvestment              0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the small transactions CSV (relative to this notebook).\n",
    "DATA_PATH = Path(\"dataset\") / \"HI-Small_Trans.csv\"\n",
    "\n",
    "# Load into a DataFrame\n",
    "small_trans = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Quick summary and preview\n",
    "print(f\"Loaded {len(small_trans)} rows; columns: {list(small_trans.columns)}\")\n",
    "small_trans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalance Visualization Overview (IBM AML Dataset)\n",
    "\n",
    "This section adds visual summaries of the strong class imbalance (fraud vs non‑fraud) and related distributions. Run in order after the dataset has been loaded into `small_trans`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "OVERALL LABEL DISTRIBUTION\n",
      "========================================================================\n",
      "Non-fraud (0)  : 5073168 (99.898%)\n",
      "Fraud (1)      :    5177 ( 0.102%)\n",
      "Fraud ratio overall: 0.00102\n",
      "\n",
      "========================================================================\n",
      "NUMERIC AMOUNT SUMMARY PER CLASS\n",
      "========================================================================\n",
      "Column: Amount Received\n",
      "                 count          mean   median           std     min           max\n",
      "Is Laundering                                                                    \n",
      "0              5073168  5.957962e+06  1407.51  1.036563e+09  0.0000  1.046302e+12\n",
      "1                 5177  3.613531e+07  8667.21  1.527919e+09  0.0032  8.485314e+10\n",
      "----------------------------------------\n",
      "Column: Amount Paid\n",
      "                 count          mean   median           std     min           max\n",
      "Is Laundering                                                                    \n",
      "0              5073168  4.477000e+06  1410.99  8.688463e+08  0.0000  1.046302e+12\n",
      "1                 5177  3.613531e+07  8667.21  1.527919e+09  0.0032  8.485314e+10\n",
      "----------------------------------------\n",
      "\n",
      "========================================================================\n",
      "TEMPORAL FRAUD RATES (first 10 windows)\n",
      "========================================================================\n",
      "Column: Amount Paid\n",
      "                 count          mean   median           std     min           max\n",
      "Is Laundering                                                                    \n",
      "0              5073168  4.477000e+06  1410.99  8.688463e+08  0.0000  1.046302e+12\n",
      "1                 5177  3.613531e+07  8667.21  1.527919e+09  0.0032  8.485314e+10\n",
      "----------------------------------------\n",
      "\n",
      "========================================================================\n",
      "TEMPORAL FRAUD RATES (first 10 windows)\n",
      "========================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29789/2048982089.py:48: FutureWarning: DataFrameGroupBy.resample operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  counts = temp_df.set_index('ts').groupby('label').resample(freq).size().unstack(0).fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using frequency: D\n",
      "            label_0  label_1    total  fraud_rate\n",
      "ts                                               \n",
      "2022-09-01  1114599      322  1114921      0.0003\n",
      "2022-09-02   754041      408   754449      0.0005\n",
      "2022-09-03   206991      391   207382      0.0019\n",
      "2022-09-04   207023      407   207430      0.0020\n",
      "2022-09-05   482179      471   482650      0.0010\n",
      "2022-09-06   481558      531   482089      0.0011\n",
      "2022-09-07   482254      497   482751      0.0010\n",
      "2022-09-08   482234      539   482773      0.0011\n",
      "2022-09-09   653953      514   654467      0.0008\n",
      "2022-09-10   207883      442   208325      0.0021\n",
      "\n",
      "========================================================================\n",
      "ACCOUNT PARTICIPATION SNAPSHOT (top 10)\n",
      "========================================================================\n",
      "Top senders by volume (From Bank):\n",
      "From Bank\n",
      "70     449859\n",
      "10      81629\n",
      "12      79754\n",
      "1       62211\n",
      "15      52511\n",
      "220     52417\n",
      "20      41008\n",
      "3       38413\n",
      "7       31086\n",
      "211     30451\n",
      "\n",
      "Top receivers by volume (To Bank):\n",
      "To Bank\n",
      "10     42547\n",
      "12     41872\n",
      "15     38721\n",
      "220    30625\n",
      "1      30115\n",
      "3      25627\n",
      "7      23029\n",
      "20     22048\n",
      "28     21160\n",
      "211    20576\n",
      "\n",
      "Top senders by fraud count:\n",
      "From Bank\n",
      "70     633\n",
      "12      76\n",
      "20      67\n",
      "119     59\n",
      "10      51\n",
      "1       50\n",
      "11      47\n",
      "15      46\n",
      "22      40\n",
      "118     36\n",
      "\n",
      "Top receivers by fraud count:\n",
      "To Bank\n",
      "12     89\n",
      "119    73\n",
      "11     68\n",
      "20     54\n",
      "1      53\n",
      "10     51\n",
      "22     48\n",
      "222    47\n",
      "23     43\n",
      "15     42\n",
      "\n",
      "\n",
      "========================================================================\n",
      "NUMERIC FEATURE CORRELATIONS WITH FRAUD (top 15 abs(r))\n",
      "========================================================================\n",
      "        feature        r  p_value\n",
      "        To Bank -0.00572  0.00000\n",
      "    Amount Paid  0.00116  0.00886\n",
      "Amount Received  0.00093  0.03640\n",
      "      From Bank -0.00023  0.60350\n",
      "        feature        r  p_value\n",
      "        To Bank -0.00572  0.00000\n",
      "    Amount Paid  0.00116  0.00886\n",
      "Amount Received  0.00093  0.03640\n",
      "      From Bank -0.00023  0.60350\n"
     ]
    }
   ],
   "source": [
    "# Text-based imbalance summary for IBM AML dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "assert 'small_trans' in globals(), \"Load the dataset into 'small_trans' first (see earlier cell).\"\n",
    "df = small_trans.copy()\n",
    "label_col = 'Is Laundering'\n",
    "if label_col not in df.columns:\n",
    "    raise KeyError(f\"Expected column '{label_col}' in the dataset.\")\n",
    "\n",
    "print('=' * 72)\n",
    "print('OVERALL LABEL DISTRIBUTION')\n",
    "print('=' * 72)\n",
    "label_counts = df[label_col].value_counts().sort_index()\n",
    "total = label_counts.sum()\n",
    "for label, count in label_counts.items():\n",
    "    pct = 100.0 * count / total\n",
    "    label_name = 'Fraud (1)' if label == 1 else 'Non-fraud (0)'\n",
    "    print(f\"{label_name:<15}: {count:>7} ({pct:6.3f}%)\")\n",
    "fraud_ratio = label_counts.get(1, 0) / max(total, 1)\n",
    "print(f\"Fraud ratio overall: {fraud_ratio:.5f}\")\n",
    "\n",
    "print('\\n' + '=' * 72)\n",
    "print('NUMERIC AMOUNT SUMMARY PER CLASS')\n",
    "print('=' * 72)\n",
    "amount_cols = [c for c in df.columns if any(k in c.lower() for k in ('amount', 'amt', 'value'))]\n",
    "if amount_cols:\n",
    "    for col in amount_cols:\n",
    "        series = pd.to_numeric(df[col], errors='coerce')\n",
    "        summary = df.groupby(label_col)[col].agg(['count', 'mean', 'median', 'std', 'min', 'max'])\n",
    "        print(f\"Column: {col}\")\n",
    "        print(summary.fillna(0).round(4).to_string())\n",
    "        print('-' * 40)\n",
    "else:\n",
    "    print('No amount-like columns detected for summary.')\n",
    "\n",
    "print('\\n' + '=' * 72)\n",
    "print('TEMPORAL FRAUD RATES (first 10 windows)')\n",
    "print('=' * 72)\n",
    "time_col = next((c for c in df.columns if any(k in c.lower() for k in ('time', 'date', 'timestamp'))), None)\n",
    "if time_col:\n",
    "    ts = pd.to_datetime(df[time_col], errors='coerce')\n",
    "    temp_df = pd.DataFrame({'ts': ts, 'label': df[label_col]}).dropna(subset=['ts'])\n",
    "    span_days = (temp_df['ts'].max() - temp_df['ts'].min()).days\n",
    "    freq = 'D' if span_days >= 2 else 'H'\n",
    "    counts = temp_df.set_index('ts').groupby('label').resample(freq).size().unstack(0).fillna(0)\n",
    "    counts.columns = [f'label_{c}' for c in counts.columns]\n",
    "    counts['total'] = counts.sum(axis=1)\n",
    "    counts['fraud_rate'] = counts.get('label_1', 0) / counts['total'].replace(0, np.nan)\n",
    "    print(f\"Using frequency: {freq}\")\n",
    "    preview = counts[['label_0', 'label_1', 'total', 'fraud_rate']].head(10).fillna(0)\n",
    "    print(preview.round({'fraud_rate': 4}).to_string())\n",
    "else:\n",
    "    print('No timestamp/date column detected for temporal summary.')\n",
    "\n",
    "print('\\n' + '=' * 72)\n",
    "print('ACCOUNT PARTICIPATION SNAPSHOT (top 10)')\n",
    "print('=' * 72)\n",
    "sender_col = next((c for c in df.columns if any(k in c.lower() for k in ('sender', 'originator', 'from', 'account'))), None)\n",
    "receiver_col = next((c for c in df.columns if any(k in c.lower() for k in ('receiver', 'beneficiary', 'to', 'account.1', 'account_1'))), None)\n",
    "if sender_col and receiver_col:\n",
    "    part_df = df[[sender_col, receiver_col, label_col]].copy()\n",
    "    top_senders = part_df.groupby(sender_col).size().sort_values(ascending=False).head(10)\n",
    "    top_receivers = part_df.groupby(receiver_col).size().sort_values(ascending=False).head(10)\n",
    "    fraud_senders = part_df.groupby(sender_col)[label_col].sum().sort_values(ascending=False).head(10)\n",
    "    fraud_receivers = part_df.groupby(receiver_col)[label_col].sum().sort_values(ascending=False).head(10)\n",
    "    print(f\"Top senders by volume ({sender_col}):\\n{top_senders.to_string()}\\n\")\n",
    "    print(f\"Top receivers by volume ({receiver_col}):\\n{top_receivers.to_string()}\\n\")\n",
    "    print(f\"Top senders by fraud count:\\n{fraud_senders.to_string()}\\n\")\n",
    "    print(f\"Top receivers by fraud count:\\n{fraud_receivers.to_string()}\\n\")\n",
    "else:\n",
    "    print('Could not identify sender/receiver columns for participation snapshot.')\n",
    "\n",
    "print('\\n' + '=' * 72)\n",
    "print('NUMERIC FEATURE CORRELATIONS WITH FRAUD (top 15 abs(r))')\n",
    "print('=' * 72)\n",
    "num_cols = [c for c in df.select_dtypes(include=[np.number]).columns if c != label_col]\n",
    "if num_cols:\n",
    "    corrs = []\n",
    "    y = df[label_col].values\n",
    "    for c in num_cols:\n",
    "        x = pd.to_numeric(df[c], errors='coerce').fillna(0).values\n",
    "        try:\n",
    "            r, p = pointbiserialr(y, x)\n",
    "        except Exception:\n",
    "            r, p = np.nan, np.nan\n",
    "        corrs.append({'feature': c, 'r': r, 'p_value': p})\n",
    "    corr_df = pd.DataFrame(corrs)\n",
    "    corr_df['abs_r'] = corr_df['r'].abs()\n",
    "    corr_df = corr_df.sort_values(by='abs_r', ascending=False).head(15)\n",
    "    print(corr_df[['feature', 'r', 'p_value']].round(5).to_string(index=False))\n",
    "else:\n",
    "    print('No numeric columns (besides label) available for correlation analysis.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# convert hex account numbers to int\n",
    "hex_to_int = np.vectorize(lambda x: int(x, 16))\n",
    "\n",
    "# create adjacency lists to represent the graph\n",
    "source = hex_to_int(small_trans['Account'])\n",
    "target = hex_to_int(small_trans['Account.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/SFSU/CSC871/csc865-anti-money-laundering-ibm/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_nodes: 515080 num_edges: 5078345\n",
      "Data(edge_index=[2, 5078345], num_nodes=515080)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "\n",
    "# Map account IDs to a compact 0..N-1 index space to avoid huge sparse IDs\n",
    "# Concatenate unique accounts from source/target and factorize\n",
    "all_accounts = np.concatenate([source, target])\n",
    "unique_accounts, inverse_idx = np.unique(all_accounts, return_inverse=True)\n",
    "num_nodes = unique_accounts.shape[0]\n",
    "# Rebuild source/target as compact indices\n",
    "source_idx = inverse_idx[:source.shape[0]]\n",
    "target_idx = inverse_idx[source.shape[0]:]\n",
    "\n",
    "# Build edge_index\n",
    "edge_index = torch.tensor(np.vstack([source_idx, target_idx]), dtype=torch.long)\n",
    "\n",
    "# Create Data object\n",
    "data = Data(edge_index=edge_index, num_nodes=num_nodes)\n",
    "print('num_nodes:', num_nodes, 'num_edges:', edge_index.size(1))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 5078345], num_nodes=515080, edge_attr=[5078345, 3], edge_label=[5078345])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "\n",
    "# extract individual edge features\n",
    "time = pd.to_datetime(small_trans['Timestamp']).astype('int64') / 1e9\n",
    "amount_paid = small_trans['Amount Paid'].to_numpy()\n",
    "amount_received = small_trans['Amount Received'].to_numpy()\n",
    "\n",
    "# combine edge features into single tensor (standardised numeric block)\n",
    "numeric_features = np.column_stack([time, amount_paid, amount_received])\n",
    "scaler = StandardScaler()\n",
    "numeric_scaled = scaler.fit_transform(numeric_features)\n",
    "edge_features = torch.from_numpy(numeric_scaled).float()\n",
    "\n",
    "# create edge labels\n",
    "fraud_label = torch.tensor(small_trans['Is Laundering'].to_numpy(), dtype=torch.long)\n",
    "\n",
    "# attach features and labels to PyG Data\n",
    "data.edge_attr = edge_features\n",
    "data.edge_label = fraud_label\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masks set: 3047007 1015669 1015669\n"
     ]
    }
   ],
   "source": [
    "# chronological 60/20/20 split by edge index order\n",
    "num_edges = data.edge_index.size(1)\n",
    "train_end = int(0.6 * num_edges)\n",
    "val_end = int(0.8 * num_edges)\n",
    "\n",
    "train_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
    "train_mask[:train_end] = True\n",
    "val_mask[train_end:val_end] = True\n",
    "test_mask[val_end:] = True\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.val_mask = val_mask\n",
    "data.test_mask = test_mask\n",
    "print('Masks set:', train_mask.sum().item(), val_mask.sum().item(), test_mask.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "# Increase edge_batch_size if you have ample memory and want fewer edge chunks per epoch.\n",
    "edge_batch_size = 1024\n",
    "# Toggle GPU usage; set to False to keep everything on CPU even if CUDA is visible.\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# Ratio of sampled negatives to each positive edge during fallback training.\n",
    "neg_pos_ratio = 2.0\n",
    "# Scale factor applied to the empirical class imbalance when computing pos_weight.\n",
    "pos_weight_scale = 0.35\n",
    "# Optional manual override for pos_weight (set to a float to force a value).\n",
    "pos_weight_override = None\n",
    "# Number of epochs to train for.\n",
    "epochs = 20\n",
    "# Hidden dimension for the GNN and edge classifier.\n",
    "num_hid = 64\n",
    "# Smaller learning rate to keep updates stable on imbalanced data.\n",
    "learn_rate = 5e-4\n",
    "# Weight decay for the optimizer.\n",
    "decay = 1e-4\n",
    "# Gradient clipping threshold (set <=0 to disable).\n",
    "grad_clip = 1.0\n",
    "# False positive rate target used when calibrating the decision threshold on validation data.\n",
    "fpr_target = 0.05\n",
    "# Minimum epochs before enabling regular recalibration so the loss can settle.\n",
    "calibrate_warmup = 6\n",
    "# How often (in epochs) to re-fit the validation ROC and refresh the threshold after warmup.\n",
    "calibrate_every = 3\n",
    "# Blend factor applied when updating the threshold (0=no change, 1=replace).\n",
    "threshold_blend = 0.2\n",
    "# Hard floor on the decision threshold to avoid runaway false positives.\n",
    "threshold_floor = 0.2\n",
    "# Hard ceiling on the decision threshold for numerical safety.\n",
    "threshold_ceiling = 0.95\n",
    "# Maximum allowed validation positive fraction before skipping a threshold update.\n",
    "max_val_pos_frac = 0.45\n",
    "# Maximum allowed validation FPR before skipping a threshold update.\n",
    "max_val_fpr = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PyG GNN model and edge classification training (batched)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import TransformerConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Ensure data object exists with edge_index, edge_attr, edge_label, and masks\n",
    "assert data is not None, 'PyG Data not constructed yet'\n",
    "num_nodes = data.num_nodes\n",
    "num_edges = data.edge_index.size(1)\n",
    "if getattr(data, 'edge_attr', None) is None:\n",
    "    raise RuntimeError('Edge features missing. Run Cell 12 (edge feature construction) before this cell.')\n",
    "edge_feat_dim = data.edge_attr.size(1)\n",
    "\n",
    "# Create simple node features if none exist (e.g., degree or identity)\n",
    "if getattr(data, 'x', None) is None:\n",
    "    deg = torch.zeros((num_nodes, 1), dtype=torch.float)\n",
    "    deg.scatter_add_(0, data.edge_index[0].view(-1,1), torch.ones((num_edges,1)))\n",
    "    data.x = deg  # use degree as a simple node feature\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, edge_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = TransformerConv(in_channels, hidden_channels, edge_dim=edge_dim)\n",
    "        self.conv2 = TransformerConv(hidden_channels, hidden_channels, edge_dim=edge_dim)\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.conv1(x, edge_index, edge_attr=edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr=edge_attr)\n",
    "        return x\n",
    "\n",
    "class EdgeClassifier(nn.Module):\n",
    "    def __init__(self, node_hidden, edge_feat_dim, hidden=num_hid):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(node_hidden*2 + edge_feat_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        u, v = edge_index\n",
    "        h = torch.cat([x[u], x[v], edge_attr], dim=1)\n",
    "        return self.mlp(h).squeeze(-1)\n",
    "    def score_pairs(self, x, u, v, edge_attr):\n",
    "        h = torch.cat([x[u], x[v], edge_attr], dim=1)\n",
    "        return self.mlp(h).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor sampling backend missing: install 'pyg-lib' (preferred) or 'torch-sparse'.\n",
      "Suggested install command (run in a terminal):\n",
      "/mnt/d/SFSU/CSC871/csc865-anti-money-laundering-ibm/.venv/bin/python -m pip install pyg-lib torch-sparse -f https://data.pyg.org/whl/torch-2.9.1+cu128.html\n",
      "Fallback to full-graph edge training will be used until a backend is installed.\n",
      "Note: pyg_lib currently does not support some newer PyTorch versions; fallback will be used if install fails.\n",
      "Using GPU device: NVIDIA GeForce RTX 5080 (index 0).\n",
      "Train edges: 3047007 | positives: 2865 (0.000940) | pos_weight 371.88 | neg/pos ratio target 2.0:1\n",
      "Fallback mode active on CUDA device: full-graph embeddings with edge chunks of 1024 (train edges 3047007, val 1015669, test 1015669).\n",
      "edge_batch_size controls chunking in this mode; install pyg-lib or torch-sparse to enable true neighbor sampling.\n",
      "Train edges: 3047007 | positives: 2865 (0.000940) | pos_weight 371.88 | neg/pos ratio target 2.0:1\n",
      "Fallback mode active on CUDA device: full-graph embeddings with edge chunks of 1024 (train edges 3047007, val 1015669, test 1015669).\n",
      "edge_batch_size controls chunking in this mode; install pyg-lib or torch-sparse to enable true neighbor sampling.\n"
     ]
    }
   ],
   "source": [
    "import importlib, math, torch, sys, subprocess, os\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "# Detect if neighbor sampling backend (pyg_lib or torch_sparse) is available\n",
    "backend_ok = bool(importlib.util.find_spec(\"pyg_lib\") or importlib.util.find_spec(\"torch_sparse\"))\n",
    "fallback_splits = {}\n",
    "if not backend_ok:\n",
    "    print(\"Neighbor sampling backend missing: install 'pyg-lib' (preferred) or 'torch-sparse'.\")\n",
    "    torch_ver = torch.__version__.split('+')[0]\n",
    "    cuda_ver = torch.version.cuda\n",
    "    if cuda_ver is None:\n",
    "        cuda_tag = 'cpu'\n",
    "    else:\n",
    "        cuda_tag = 'cu' + cuda_ver.replace('.', '')\n",
    "    index_url = f'https://data.pyg.org/whl/torch-{torch_ver}+{cuda_tag}.html'\n",
    "    print('Suggested install command (run in a terminal):')\n",
    "    print(f\"{sys.executable} -m pip install pyg-lib torch-sparse -f {index_url}\")\n",
    "    print('Fallback to full-graph edge training will be used until a backend is installed.')\n",
    "    print('Note: pyg_lib currently does not support some newer PyTorch versions; fallback will be used if install fails.')\n",
    "\n",
    "requested_gpu = use_gpu and torch.cuda.is_available()\n",
    "device = torch.device('cuda') if requested_gpu else torch.device('cpu')\n",
    "if device.type == 'cuda':\n",
    "    dev_index = device.index if device.index is not None else torch.cuda.current_device()\n",
    "    print(f'Using GPU device: {torch.cuda.get_device_name(dev_index)} (index {dev_index}).')\n",
    "else:\n",
    "    if use_gpu and not torch.cuda.is_available():\n",
    "        print('CUDA requested but not available; falling back to CPU.')\n",
    "    else:\n",
    "        print('Using CPU for training (set use_gpu=True and ensure CUDA availability to use GPU).')\n",
    "\n",
    "gnn = GNN(in_channels=data.x.size(1), hidden_channels=num_hid, edge_dim=edge_feat_dim).to(device)\n",
    "clf = EdgeClassifier(node_hidden=num_hid, edge_feat_dim=edge_feat_dim, hidden=num_hid*2).to(device)\n",
    "\n",
    "params = list(gnn.parameters()) + list(clf.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=learn_rate, weight_decay=decay)\n",
    "\n",
    "# Build edge_label_index and edge_label tensors for each split\n",
    "train_edge_label_index = data.edge_index[:, data.train_mask]\n",
    "train_edge_label = data.edge_label[data.train_mask]\n",
    "\n",
    "val_edge_label_index = data.edge_index[:, data.val_mask]\n",
    "val_edge_label = data.edge_label[data.val_mask]\n",
    "\n",
    "test_edge_label_index = data.edge_index[:, data.test_mask]\n",
    "test_edge_label = data.edge_label[data.test_mask]\n",
    "\n",
    "train_pos = int(train_edge_label.sum().item())\n",
    "train_total = int(train_edge_label.numel())\n",
    "train_neg = max(train_total - train_pos, 0)\n",
    "base_pos_weight = (train_neg / max(train_pos, 1)) if train_pos > 0 else 1.0\n",
    "if pos_weight_override is not None:\n",
    "    pos_weight_value = float(pos_weight_override)\n",
    "else:\n",
    "    pos_weight_value = max(base_pos_weight * pos_weight_scale, 1.0)\n",
    "pos_weight_tensor = torch.tensor(pos_weight_value, dtype=torch.float, device=device)\n",
    "print(f'Train edges: {train_total} | positives: {train_pos} ({train_pos / max(train_total,1):.6f}) | pos_weight {pos_weight_value:.2f} | neg/pos ratio target {neg_pos_ratio:.1f}:1')\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "\n",
    "# Safer defaults for batch size & neighbors to reduce per-batch time/memory\n",
    "batch_size = edge_batch_size  # configurable via hyperparameter cell\n",
    "num_neighbors = [10, 5]  # fewer neighbors for smaller subgraphs\n",
    "\n",
    "fallback_mode = not backend_ok\n",
    "if backend_ok:\n",
    "    print(f'Backend OK: {backend_ok} | batch_size: {batch_size} | num_neighbors: {num_neighbors}')\n",
    "    train_loader = LinkNeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=num_neighbors,\n",
    "        batch_size=batch_size,\n",
    "        edge_label_index=train_edge_label_index,\n",
    "        edge_label=train_edge_label,\n",
    "        shuffle=True,\n",
    "        neg_sampling_ratio=0.0\n",
    "    )\n",
    "    val_loader = LinkNeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=num_neighbors,\n",
    "        batch_size=batch_size,\n",
    "        edge_label_index=val_edge_label_index,\n",
    "        edge_label=val_edge_label,\n",
    "        shuffle=False,\n",
    "        neg_sampling_ratio=0.0\n",
    "    )\n",
    "    test_loader = LinkNeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=num_neighbors,\n",
    "        batch_size=batch_size,\n",
    "        edge_label_index=test_edge_label_index,\n",
    "        edge_label=test_edge_label,\n",
    "        shuffle=False,\n",
    "        neg_sampling_ratio=0.0\n",
    "    )\n",
    "else:\n",
    "    def _split_edges(mask):\n",
    "        return {\n",
    "            'edge_label_index': data.edge_index[:, mask],\n",
    "            'edge_label': data.edge_label[mask],\n",
    "            'edge_attr': data.edge_attr[mask]\n",
    "        }\n",
    "    fallback_splits = {\n",
    "        'train': _split_edges(data.train_mask),\n",
    "        'val': _split_edges(data.val_mask),\n",
    "        'test': _split_edges(data.test_mask)\n",
    "    }\n",
    "    train_loader = fallback_splits['train']\n",
    "    val_loader = fallback_splits['val']\n",
    "    test_loader = fallback_splits['test']\n",
    "    train_count = fallback_splits['train']['edge_label'].numel()\n",
    "    val_count = fallback_splits['val']['edge_label'].numel()\n",
    "    test_count = fallback_splits['test']['edge_label'].numel()\n",
    "    chunk_size = max(int(edge_batch_size), 1)\n",
    "    print(f'Fallback mode active on {device.type.upper()} device: full-graph embeddings with edge chunks of {chunk_size} (train edges {train_count}, val {val_count}, test {test_count}).')\n",
    "    print('edge_batch_size controls chunking in this mode; install pyg-lib or torch-sparse to enable true neighbor sampling.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss 95494.3359 | time 1.4s | train_acc 0.408 | val_acc 0.799 | val_precision 0.001 | val_recall 0.221 | val_f1 0.002 | val_fpr 0.2007 | val_thresh 0.9500 | val_pos_frac 0.20071\n",
      "Epoch 02 | loss 37108.8008 | time 0.4s | train_acc 0.437 | val_acc 0.361 | val_precision 0.001 | val_recall 0.520 | val_f1 0.002 | val_fpr 0.6389 | val_thresh 0.9500 | val_pos_frac 0.63876\n",
      "Epoch 02 | loss 37108.8008 | time 0.4s | train_acc 0.437 | val_acc 0.361 | val_precision 0.001 | val_recall 0.520 | val_f1 0.002 | val_fpr 0.6389 | val_thresh 0.9500 | val_pos_frac 0.63876\n",
      "Epoch 03 | loss 11435.7354 | time 0.4s | train_acc 0.200 | val_acc 0.088 | val_precision 0.001 | val_recall 0.771 | val_f1 0.002 | val_fpr 0.9130 | val_thresh 0.9500 | val_pos_frac 0.91287\n",
      "Epoch 03 | loss 11435.7354 | time 0.4s | train_acc 0.200 | val_acc 0.088 | val_precision 0.001 | val_recall 0.771 | val_f1 0.002 | val_fpr 0.9130 | val_thresh 0.9500 | val_pos_frac 0.91287\n",
      "Epoch 04 | loss 1139.3503 | time 0.5s | train_acc 0.188 | val_acc 0.074 | val_precision 0.001 | val_recall 0.772 | val_f1 0.002 | val_fpr 0.9265 | val_thresh 0.9500 | val_pos_frac 0.92637\n",
      "Epoch 04 | loss 1139.3503 | time 0.5s | train_acc 0.188 | val_acc 0.074 | val_precision 0.001 | val_recall 0.772 | val_f1 0.002 | val_fpr 0.9265 | val_thresh 0.9500 | val_pos_frac 0.92637\n",
      "Epoch 05 | loss 1543.7103 | time 0.4s | train_acc 0.172 | val_acc 0.058 | val_precision 0.001 | val_recall 0.778 | val_f1 0.002 | val_fpr 0.9430 | val_thresh 0.9500 | val_pos_frac 0.94285\n",
      "Epoch 05 | loss 1543.7103 | time 0.4s | train_acc 0.172 | val_acc 0.058 | val_precision 0.001 | val_recall 0.778 | val_f1 0.002 | val_fpr 0.9430 | val_thresh 0.9500 | val_pos_frac 0.94285\n",
      "  skip threshold update: val_fpr 0.435 exceeds 0.250\n",
      "Epoch 06 | loss 1204.5819 | time 0.4s | train_acc 0.184 | val_acc 0.564 | val_precision 0.001 | val_recall 0.395 | val_f1 0.002 | val_fpr 0.4354 | val_thresh 0.9500 | val_pos_frac 0.43532\n",
      "  skip threshold update: val_fpr 0.435 exceeds 0.250\n",
      "Epoch 06 | loss 1204.5819 | time 0.4s | train_acc 0.184 | val_acc 0.564 | val_precision 0.001 | val_recall 0.395 | val_f1 0.002 | val_fpr 0.4354 | val_thresh 0.9500 | val_pos_frac 0.43532\n",
      "Epoch 07 | loss 1037.7812 | time 0.4s | train_acc 0.208 | val_acc 0.092 | val_precision 0.001 | val_recall 0.756 | val_f1 0.002 | val_fpr 0.9086 | val_thresh 0.9500 | val_pos_frac 0.90845\n",
      "Epoch 07 | loss 1037.7812 | time 0.4s | train_acc 0.208 | val_acc 0.092 | val_precision 0.001 | val_recall 0.756 | val_f1 0.002 | val_fpr 0.9086 | val_thresh 0.9500 | val_pos_frac 0.90845\n",
      "Epoch 08 | loss 1035.8593 | time 0.4s | train_acc 0.268 | val_acc 0.152 | val_precision 0.001 | val_recall 0.693 | val_f1 0.002 | val_fpr 0.8485 | val_thresh 0.9500 | val_pos_frac 0.84836\n",
      "Epoch 08 | loss 1035.8593 | time 0.4s | train_acc 0.268 | val_acc 0.152 | val_precision 0.001 | val_recall 0.693 | val_f1 0.002 | val_fpr 0.8485 | val_thresh 0.9500 | val_pos_frac 0.84836\n",
      "  skip threshold update: val_fpr 0.372 exceeds 0.250\n",
      "Epoch 09 | loss 902.1168 | time 0.4s | train_acc 0.420 | val_acc 0.628 | val_precision 0.001 | val_recall 0.383 | val_f1 0.002 | val_fpr 0.3719 | val_thresh 0.9500 | val_pos_frac 0.37194\n",
      "  skip threshold update: val_fpr 0.372 exceeds 0.250\n",
      "Epoch 09 | loss 902.1168 | time 0.4s | train_acc 0.420 | val_acc 0.628 | val_precision 0.001 | val_recall 0.383 | val_f1 0.002 | val_fpr 0.3719 | val_thresh 0.9500 | val_pos_frac 0.37194\n",
      "Epoch 10 | loss 7810.6084 | time 0.4s | train_acc 0.532 | val_acc 0.451 | val_precision 0.001 | val_recall 0.409 | val_f1 0.002 | val_fpr 0.5490 | val_thresh 0.9500 | val_pos_frac 0.54881\n",
      "Epoch 10 | loss 7810.6084 | time 0.4s | train_acc 0.532 | val_acc 0.451 | val_precision 0.001 | val_recall 0.409 | val_f1 0.002 | val_fpr 0.5490 | val_thresh 0.9500 | val_pos_frac 0.54881\n",
      "Epoch 11 | loss 26851.6836 | time 0.4s | train_acc 0.548 | val_acc 0.468 | val_precision 0.001 | val_recall 0.402 | val_f1 0.002 | val_fpr 0.5315 | val_thresh 0.9500 | val_pos_frac 0.53134\n",
      "Epoch 11 | loss 26851.6836 | time 0.4s | train_acc 0.548 | val_acc 0.468 | val_precision 0.001 | val_recall 0.402 | val_f1 0.002 | val_fpr 0.5315 | val_thresh 0.9500 | val_pos_frac 0.53134\n",
      "  skip threshold update: val_fpr 0.298 exceeds 0.250\n",
      "Epoch 12 | loss 31115.0176 | time 0.4s | train_acc 0.534 | val_acc 0.701 | val_precision 0.001 | val_recall 0.300 | val_f1 0.002 | val_fpr 0.2981 | val_thresh 0.9500 | val_pos_frac 0.29815\n",
      "  skip threshold update: val_fpr 0.298 exceeds 0.250\n",
      "Epoch 12 | loss 31115.0176 | time 0.4s | train_acc 0.534 | val_acc 0.701 | val_precision 0.001 | val_recall 0.300 | val_f1 0.002 | val_fpr 0.2981 | val_thresh 0.9500 | val_pos_frac 0.29815\n",
      "Epoch 13 | loss 19438.2266 | time 0.4s | train_acc 0.464 | val_acc 0.362 | val_precision 0.001 | val_recall 0.539 | val_f1 0.002 | val_fpr 0.6378 | val_thresh 0.9500 | val_pos_frac 0.63772\n",
      "Epoch 13 | loss 19438.2266 | time 0.4s | train_acc 0.464 | val_acc 0.362 | val_precision 0.001 | val_recall 0.539 | val_f1 0.002 | val_fpr 0.6378 | val_thresh 0.9500 | val_pos_frac 0.63772\n",
      "Epoch 14 | loss 6526.1416 | time 0.4s | train_acc 0.393 | val_acc 0.278 | val_precision 0.001 | val_recall 0.600 | val_f1 0.002 | val_fpr 0.7228 | val_thresh 0.9500 | val_pos_frac 0.72264\n",
      "Epoch 14 | loss 6526.1416 | time 0.4s | train_acc 0.393 | val_acc 0.278 | val_precision 0.001 | val_recall 0.600 | val_f1 0.002 | val_fpr 0.7228 | val_thresh 0.9500 | val_pos_frac 0.72264\n",
      "  skip threshold update: val_fpr 0.447 exceeds 0.250\n",
      "Epoch 15 | loss 887.2313 | time 0.4s | train_acc 0.341 | val_acc 0.553 | val_precision 0.001 | val_recall 0.466 | val_f1 0.002 | val_fpr 0.4466 | val_thresh 0.9500 | val_pos_frac 0.44662\n",
      "  skip threshold update: val_fpr 0.447 exceeds 0.250\n",
      "Epoch 15 | loss 887.2313 | time 0.4s | train_acc 0.341 | val_acc 0.553 | val_precision 0.001 | val_recall 0.466 | val_f1 0.002 | val_fpr 0.4466 | val_thresh 0.9500 | val_pos_frac 0.44662\n",
      "Epoch 16 | loss 749.3104 | time 0.4s | train_acc 0.338 | val_acc 0.228 | val_precision 0.001 | val_recall 0.669 | val_f1 0.002 | val_fpr 0.7725 | val_thresh 0.9500 | val_pos_frac 0.77242\n",
      "Epoch 16 | loss 749.3104 | time 0.4s | train_acc 0.338 | val_acc 0.228 | val_precision 0.001 | val_recall 0.669 | val_f1 0.002 | val_fpr 0.7725 | val_thresh 0.9500 | val_pos_frac 0.77242\n",
      "Epoch 17 | loss 759.4349 | time 0.4s | train_acc 0.375 | val_acc 0.270 | val_precision 0.001 | val_recall 0.643 | val_f1 0.002 | val_fpr 0.7307 | val_thresh 0.9500 | val_pos_frac 0.73064\n",
      "Epoch 17 | loss 759.4349 | time 0.4s | train_acc 0.375 | val_acc 0.270 | val_precision 0.001 | val_recall 0.643 | val_f1 0.002 | val_fpr 0.7307 | val_thresh 0.9500 | val_pos_frac 0.73064\n",
      "  skip threshold update: val_fpr 0.264 exceeds 0.250\n",
      "Epoch 18 | loss 607.0780 | time 0.4s | train_acc 0.616 | val_acc 0.736 | val_precision 0.001 | val_recall 0.320 | val_f1 0.003 | val_fpr 0.2639 | val_thresh 0.9500 | val_pos_frac 0.26398\n",
      "  skip threshold update: val_fpr 0.264 exceeds 0.250\n",
      "Epoch 18 | loss 607.0780 | time 0.4s | train_acc 0.616 | val_acc 0.736 | val_precision 0.001 | val_recall 0.320 | val_f1 0.003 | val_fpr 0.2639 | val_thresh 0.9500 | val_pos_frac 0.26398\n",
      "Epoch 19 | loss 6804.7339 | time 0.4s | train_acc 0.657 | val_acc 0.609 | val_precision 0.001 | val_recall 0.443 | val_f1 0.002 | val_fpr 0.3912 | val_thresh 0.9500 | val_pos_frac 0.39125\n",
      "Epoch 19 | loss 6804.7339 | time 0.4s | train_acc 0.657 | val_acc 0.609 | val_precision 0.001 | val_recall 0.443 | val_f1 0.002 | val_fpr 0.3912 | val_thresh 0.9500 | val_pos_frac 0.39125\n",
      "Epoch 20 | loss 11015.9512 | time 0.4s | train_acc 0.659 | val_acc 0.611 | val_precision 0.001 | val_recall 0.449 | val_f1 0.002 | val_fpr 0.3888 | val_thresh 0.9500 | val_pos_frac 0.38891\n",
      "Epoch 20 | loss 11015.9512 | time 0.4s | train_acc 0.659 | val_acc 0.611 | val_precision 0.001 | val_recall 0.449 | val_f1 0.002 | val_fpr 0.3888 | val_thresh 0.9500 | val_pos_frac 0.38891\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, roc_curve\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import gc\n",
    "from contextlib import nullcontext\n",
    "\n",
    "\n",
    "def _gather_label_edge_attr(batch):\n",
    "    # Map local edges (u,v) -> position in batch.edge_index to gather edge_attr for labeled edges\n",
    "    e_u = batch.edge_index[0].tolist()\n",
    "    e_v = batch.edge_index[1].tolist()\n",
    "    pos_map = {(eu, ev): i for i, (eu, ev) in enumerate(zip(e_u, e_v))}\n",
    "    lu = batch.edge_label_index[0].tolist()\n",
    "    lv = batch.edge_label_index[1].tolist()\n",
    "    idx = [pos_map[(u, v)] for u, v in zip(lu, lv)]\n",
    "    return batch.edge_attr[idx]\n",
    "\n",
    "\n",
    "def _iter_fallback_chunks(split, chunk_size, index_subset=None):\n",
    "    edge_index = split['edge_label_index']\n",
    "    edge_attr = split['edge_attr']\n",
    "    edge_label = split['edge_label']\n",
    "    if index_subset is None:\n",
    "        indices = torch.arange(edge_label.size(0))\n",
    "    else:\n",
    "        indices = index_subset\n",
    "    total = indices.numel()\n",
    "    for start in range(0, total, chunk_size):\n",
    "        sel = indices[start:min(start + chunk_size, total)]\n",
    "        yield edge_index[:, sel], edge_attr[sel], edge_label[sel]\n",
    "\n",
    "\n",
    "def _sample_balanced_indices(labels, ratio):\n",
    "    pos_idx = torch.nonzero(labels == 1, as_tuple=False).view(-1)\n",
    "    neg_idx = torch.nonzero(labels == 0, as_tuple=False).view(-1)\n",
    "    if pos_idx.numel() == 0:\n",
    "        return torch.zeros(0, dtype=torch.long)\n",
    "    neg_needed = int(math.ceil(pos_idx.numel() * ratio))\n",
    "    if neg_idx.numel() == 0:\n",
    "        combined = pos_idx\n",
    "    else:\n",
    "        neg_needed = min(max(neg_needed, pos_idx.numel()), neg_idx.numel())\n",
    "        perm = torch.randperm(neg_idx.numel())\n",
    "        sampled_neg = neg_idx[perm[:neg_needed]]\n",
    "        combined = torch.cat([pos_idx, sampled_neg])\n",
    "    shuffle = torch.randperm(combined.numel())\n",
    "    return combined[shuffle]\n",
    "\n",
    "\n",
    "def _select_threshold(y_true, probs, target_fpr=0.02):\n",
    "    if y_true.size == 0:\n",
    "        return 0.5\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, probs)\n",
    "    if np.isnan(thresholds).all():\n",
    "        return 0.5\n",
    "    # Remove infinities for stability\n",
    "    finite_mask = np.isfinite(thresholds)\n",
    "    fpr, tpr, thresholds = fpr[finite_mask], tpr[finite_mask], thresholds[finite_mask]\n",
    "    if thresholds.size == 0:\n",
    "        return 0.5\n",
    "    if target_fpr is not None:\n",
    "        ok = np.where(fpr <= target_fpr)[0]\n",
    "        if ok.size > 0:\n",
    "            idx = ok[np.argmax(tpr[ok])]\n",
    "        else:\n",
    "            idx = np.argmin(fpr)\n",
    "    else:\n",
    "        youden = tpr - fpr\n",
    "        idx = np.argmax(youden)\n",
    "    thr = thresholds[idx]\n",
    "    if np.isnan(thr):\n",
    "        thr = 0.5\n",
    "    return float(np.clip(thr, 1e-6, 1 - 1e-6))\n",
    "\n",
    "\n",
    "def _summarise_predictions(y_true, probs, threshold):\n",
    "    preds = (probs >= threshold).astype(np.int64)\n",
    "    if preds.size == 0:\n",
    "        return {\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1': 0.0,\n",
    "            'fpr': 0.0,\n",
    "            'acc': 0.0,\n",
    "            'pos_frac': 0.0,\n",
    "            'threshold': threshold,\n",
    "            'preds': preds,\n",
    "            'probs': probs,\n",
    "            'labels': y_true\n",
    "        }\n",
    "    acc = (preds == y_true).mean()\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(y_true, preds, average='binary', zero_division=0)\n",
    "    cm = confusion_matrix(y_true, preds, labels=[0, 1])\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        fpr = fp / max(tn + fp, 1)\n",
    "    else:\n",
    "        fpr = 0.0\n",
    "    pos_frac = preds.mean()\n",
    "    return {\n",
    "        'precision': float(pr),\n",
    "        'recall': float(rc),\n",
    "        'f1': float(f1),\n",
    "        'fpr': float(fpr),\n",
    "        'acc': float(acc),\n",
    "        'pos_frac': float(pos_frac),\n",
    "        'threshold': float(threshold),\n",
    "        'preds': preds,\n",
    "        'probs': probs,\n",
    "        'labels': y_true\n",
    "    }\n",
    "\n",
    "\n",
    "def train_one_epoch(use_amp=True, log_every=200, neg_ratio=3.0):\n",
    "    gnn.train(); clf.train()\n",
    "    amp_enabled = use_amp and (device.type == 'cuda') and not fallback_mode\n",
    "    scaler = torch.amp.GradScaler('cuda') if amp_enabled else None\n",
    "    clip_enabled = (grad_clip is not None) and (grad_clip > 0)\n",
    "    if fallback_mode:\n",
    "        optimizer.zero_grad()\n",
    "        x = gnn(data.x.to(device), data.edge_index.to(device), data.edge_attr.to(device))\n",
    "        split = fallback_splits['train']\n",
    "        labels_cpu = split['edge_label'].cpu()\n",
    "        selected_indices = _sample_balanced_indices(labels_cpu, neg_ratio)\n",
    "        if selected_indices.numel() == 0:\n",
    "            print('No positive edges found in training split; cannot update model.')\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            return float('nan')\n",
    "        total_edges = selected_indices.numel()\n",
    "        chunk_size = max(int(edge_batch_size), 1)\n",
    "        loss_terms = []\n",
    "        for edge_idx_chunk, edge_attr_chunk, label_chunk in _iter_fallback_chunks(split, chunk_size, selected_indices):\n",
    "            u = edge_idx_chunk[0].to(device)\n",
    "            v = edge_idx_chunk[1].to(device)\n",
    "            edge_attr = torch.nan_to_num(edge_attr_chunk, nan=0.0, posinf=0.0, neginf=0.0).to(device)\n",
    "            edge_label = label_chunk.to(device).float()\n",
    "            logits = clf.score_pairs(x, u, v, edge_attr)\n",
    "            loss = criterion(logits, edge_label)\n",
    "            if not torch.isfinite(loss):\n",
    "                print('Non-finite loss encountered in fallback chunk; try lowering pos_weight or learning rate.')\n",
    "                return float('nan')\n",
    "            loss_terms.append(loss * edge_label.numel())\n",
    "        if not loss_terms:\n",
    "            print('Balanced sampling produced no batches; skipping epoch.')\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            return 0.0\n",
    "        loss_total = torch.stack(loss_terms).sum() / max(total_edges, 1)\n",
    "        loss_total.backward()\n",
    "        if clip_enabled:\n",
    "            torch.nn.utils.clip_grad_norm_(params, max_norm=grad_clip)\n",
    "        optimizer.step()\n",
    "        return float(loss_total.detach().cpu())\n",
    "    # Neighbor-sampling path (backend available)\n",
    "    total_loss = 0.0\n",
    "    total_count = 0\n",
    "    t0 = time.time()\n",
    "    for i, batch in enumerate(tqdm(train_loader, desc='train_batches'), 1):\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "        context = torch.amp.autocast('cuda') if scaler is not None else nullcontext()\n",
    "        with context:\n",
    "            x = gnn(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            label_edge_attr = _gather_label_edge_attr(batch)\n",
    "            logits = clf.score_pairs(x, batch.edge_label_index[0], batch.edge_label_index[1], label_edge_attr)\n",
    "            labels_float = batch.edge_label.float()\n",
    "            loss = criterion(logits, labels_float)\n",
    "        if not torch.isfinite(loss):\n",
    "            print(f'  batch {i} produced non-finite loss; skipping update.')\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            continue\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            if clip_enabled:\n",
    "                torch.nn.utils.clip_grad_norm_(params, max_norm=grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            if clip_enabled:\n",
    "                torch.nn.utils.clip_grad_norm_(params, max_norm=grad_clip)\n",
    "            optimizer.step()\n",
    "        batch_size_local = batch.edge_label.numel()\n",
    "        total_loss += loss.item() * batch_size_local\n",
    "        total_count += batch_size_local\n",
    "        if i % max(log_every, 1) == 0:\n",
    "            print(f'  batch {i} | batch_loss {loss.item():.4f} | elapsed {time.time()-t0:.1f}s')\n",
    "    return total_loss / max(total_count, 1)\n",
    "\n",
    "\n",
    "def evaluate_split(split_name, threshold=None, calibrate=False):\n",
    "    gnn.eval(); clf.eval()\n",
    "    chunk_size = max(int(edge_batch_size), 1)\n",
    "    labels_list = []\n",
    "    probs_list = []\n",
    "    if fallback_mode:\n",
    "        split = fallback_splits[split_name]\n",
    "        with torch.no_grad():\n",
    "            x = gnn(data.x.to(device), data.edge_index.to(device), data.edge_attr.to(device))\n",
    "            for edge_idx_chunk, edge_attr_chunk, label_chunk in _iter_fallback_chunks(split, chunk_size):\n",
    "                u = edge_idx_chunk[0].to(device)\n",
    "                v = edge_idx_chunk[1].to(device)\n",
    "                edge_attr = torch.nan_to_num(edge_attr_chunk, nan=0.0, posinf=0.0, neginf=0.0).to(device)\n",
    "                logits = clf.score_pairs(x, u, v, edge_attr)\n",
    "                probs = torch.sigmoid(logits).detach().cpu()\n",
    "                probs_list.append(probs)\n",
    "                labels_list.append(label_chunk.detach().cpu())\n",
    "    else:\n",
    "        loader_map = {'train': train_loader, 'val': val_loader, 'test': test_loader}\n",
    "        loader = loader_map[split_name]\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(loader, desc=f'{split_name}_batches'):\n",
    "                batch = batch.to(device)\n",
    "                x = gnn(batch.x, batch.edge_index, batch.edge_attr)\n",
    "                label_edge_attr = _gather_label_edge_attr(batch)\n",
    "                logits = clf.score_pairs(x, batch.edge_label_index[0], batch.edge_label_index[1], label_edge_attr)\n",
    "                probs = torch.sigmoid(logits).detach().cpu()\n",
    "                probs_list.append(probs)\n",
    "                labels_list.append(batch.edge_label.detach().cpu())\n",
    "    if not labels_list:\n",
    "        empty = np.array([])\n",
    "        return {\n",
    "            'acc': 0.0,\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1': 0.0,\n",
    "            'fpr': 0.0,\n",
    "            'pos_frac': 0.0,\n",
    "            'threshold': threshold if threshold is not None else 0.5,\n",
    "            'preds': empty,\n",
    "            'probs': empty,\n",
    "            'labels': empty\n",
    "        }\n",
    "    labels = torch.cat(labels_list).numpy().astype(np.int64)\n",
    "    probs = torch.cat(probs_list).numpy().astype(np.float32)\n",
    "    if calibrate or threshold is None:\n",
    "        chosen_threshold = _select_threshold(labels, probs, target_fpr=fpr_target)\n",
    "    else:\n",
    "        chosen_threshold = threshold\n",
    "    summary = _summarise_predictions(labels, probs, chosen_threshold)\n",
    "    return summary\n",
    "\n",
    "\n",
    "log_every = 200 if not fallback_mode else 0\n",
    "calibrated_threshold = 0.5\n",
    "for epoch in range(1, epochs+1):\n",
    "    epoch_t0 = time.time()\n",
    "    avg_loss = train_one_epoch(use_amp=(device.type == 'cuda'), log_every=log_every, neg_ratio=neg_pos_ratio)\n",
    "    epoch_time = time.time() - epoch_t0\n",
    "    if not math.isfinite(avg_loss):\n",
    "        print(f'Epoch {epoch:02d} skipped due to non-finite loss.')\n",
    "        continue\n",
    "    warmup_ready = epoch >= calibrate_warmup\n",
    "    should_calibrate = (epoch == 1) or (warmup_ready and ((epoch - calibrate_warmup) % max(calibrate_every, 1) == 0))\n",
    "    train_metrics = evaluate_split('train', threshold=calibrated_threshold)\n",
    "    val_metrics = evaluate_split('val', threshold=calibrated_threshold, calibrate=should_calibrate)\n",
    "    if should_calibrate:\n",
    "        new_threshold = val_metrics['threshold']\n",
    "        allow_update = np.isfinite(new_threshold)\n",
    "        if allow_update and val_metrics['pos_frac'] > max_val_pos_frac:\n",
    "            print(f\"  skip threshold update: val_pos_frac {val_metrics['pos_frac']:.3f} exceeds {max_val_pos_frac:.3f}\")\n",
    "            allow_update = False\n",
    "        if allow_update and val_metrics['fpr'] > max_val_fpr:\n",
    "            print(f\"  skip threshold update: val_fpr {val_metrics['fpr']:.3f} exceeds {max_val_fpr:.3f}\")\n",
    "            allow_update = False\n",
    "        if allow_update:\n",
    "            if epoch == 1:\n",
    "                calibrated_threshold = float(np.clip(new_threshold, threshold_floor, threshold_ceiling))\n",
    "            else:\n",
    "                blended = ((1.0 - threshold_blend) * calibrated_threshold) + (threshold_blend * new_threshold)\n",
    "                calibrated_threshold = float(np.clip(blended, threshold_floor, threshold_ceiling))\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | loss {avg_loss:.4f} | time {epoch_time:.1f}s | \"\n",
    "        f\"train_acc {train_metrics['acc']:.3f} | val_acc {val_metrics['acc']:.3f} | \"\n",
    "        f\"val_precision {val_metrics['precision']:.3f} | val_recall {val_metrics['recall']:.3f} | \"\n",
    "        f\"val_f1 {val_metrics['f1']:.3f} | val_fpr {val_metrics['fpr']:.4f} | val_thresh {calibrated_threshold:.4f} | \"\n",
    "        f\"val_pos_frac {val_metrics['pos_frac']:.5f}\"\n",
    "    )\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.628514\n",
      "Test precision: 0.001472\n",
      "Test recall: 0.443645\n",
      "Test F1: 0.002933\n",
      "False positive rate: 0.371258\n",
      "Predicted positive fraction: 0.371347\n",
      "Decision threshold: 0.95\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "test_metrics = evaluate_split('test', threshold=calibrated_threshold)\n",
    "labels = test_metrics['labels']\n",
    "preds = test_metrics['preds']\n",
    "print('Test accuracy:', round(test_metrics['acc'], 6))\n",
    "print('Test precision:', round(test_metrics['precision'], 6))\n",
    "print('Test recall:', round(test_metrics['recall'], 6))\n",
    "print('Test F1:', round(test_metrics['f1'], 6))\n",
    "print('False positive rate:', round(test_metrics['fpr'], 6))\n",
    "print('Predicted positive fraction:', round(test_metrics['pos_frac'], 6))\n",
    "print('Decision threshold:', round(test_metrics['threshold'], 6))\n",
    "\n",
    "if labels.size > 0 and preds.size > 0:\n",
    "    cm = confusion_matrix(labels, preds, labels=[0, 1])\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=['Non-fraud', 'Fraud'])\n",
    "    disp.plot(values_format='d')\n",
    "else:\n",
    "    print('Not enough classes in test to compute CM/recall/FPR.')\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOv070Al0cK/8N1oI0v7dEu",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "csc865-anti-money-laundering-ibm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
